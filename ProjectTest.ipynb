{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/46950173/python-looping-through-directory-and-saving-each-file-using-filename-as-data-fr\n",
    "#use to iterate through folder with each of the csv and create a  dataframe for each\n",
    "root = 'C:\\\\Users\\\\Olivia\\\\Documents'\n",
    "\n",
    "ddict = {}\n",
    "for file in os.listdir(root):\n",
    "    if file.endswith(\".csv\"):\n",
    "        name = os.path.splitext(file)[0]\n",
    "        ddict[name] = pd.read_csv(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I do all the rest of this as an iterator? b/c we have at least 58 files to start off with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perhaps rename each ddict[name] as it's own month or year? not sure if i'll be able to edit each file for what I need to do following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes start time from type 0 to date time\n",
    "trip_df['o_time'] =pd.to_datetime(trip_df['starttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the hour for date time\n",
    "trip_df['o_hourofday']=trip_df['o_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the day of the week in order to create tables per hour\n",
    "trip_df['o_dayofweek']=trip_df['o_time'].dt.day_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unique route signifier to be able to use later\n",
    "trip_df['trip']=trip_df['start station id'].map(str)+\" \"+trip_df['end station id'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split table for age group\n",
    "date = pd.datetime.now().year\n",
    "trip_df['age'] = date - trip_df['birth year']\n",
    "BabyBoomer = trip_df[trip_df['age']>=59]\n",
    "GenX = trip_df[(trip_df['age']>=39)&(trip_df['age']<=58)]\n",
    "GenY = trip_df[(trip_df['age']>=23)&(trip_df['age']<=38)]\n",
    "GenZ = trip_df[trip_df['age']<=22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split table by usertype\n",
    "#ddict[name] is to call the specific table file - need to find out if that's the right way\n",
    "subscriber = [ddict[name]['usertype']=='Subscriber']\n",
    "customer = [ddict[name]['usertype']=='Customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split table by day of week\n",
    "weekday = subscriber['o_dayofweek']=='Monday' or 'Tuesday' or 'Wednesday' or 'Thursday' or 'Friday']\n",
    "weekend = customer['o_dayofweek']=='Saturday' or 'Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split table by time period for weekday, based off bike allowance times on MBTA\n",
    "morning = [weekday['o_hourofday']=='0' or '1' or '2' or '3' or '4' or '5' or '6']\n",
    "morncommute = [weekday['o_hourofday']=='7' or '8' or '9' or '10']\n",
    "midday = [weekday['o_hourofday']=='11' or '12' or '13' or '14' or '15']\n",
    "evecommute = [weekday['o_hourofday']=='16' or '17' or '18' or '19']\n",
    "night = [weekday['o_hourofday']=='20' or '21' or '22' or '23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps do group by for each table for trip\n",
    "trip_df.groupby('trip').count()\n",
    "#Or\n",
    "count=trip_df['trip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if using count (not groupby)- create dateframe then need to add column heads.\n",
    "df = pd.DataFrame(count, columns = ['trip', 'weight']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split trip into source and end\n",
    "#https://www.geeksforgeeks.org/python-pandas-split-strings-into-two-list-columns-using-str-split/\n",
    "  \n",
    "# new data frame with split value columns - this is an interim df\n",
    "new = df[\"trip\"].str.split(\" \", n = 1, expand = True) \n",
    "  \n",
    "# making separate source column from new data frame in the original\n",
    "df[\"source\"]= new[0] \n",
    "  \n",
    "# making separate end column from new data frame \n",
    "df[\"target\"]= new[1] \n",
    "  \n",
    "# Dropping old trip columns \n",
    "df.drop(columns =[\"Trip\"], inplace = True) \n",
    "  \n",
    "# df display \n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#May need to clear table of anything we dont need at this point. Probably only want table to include \n",
    "Source ID End ID Weight (number of trips in month)\n",
    "? How do I add the location - does this mean I need to use OSnx - perhaps, becasue the source ID and end ID aren't going to help with geolocation and will create an aspatial map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to do some creation of tables for simple data analysis. possibly look at the percentage of trips that happen to determine an average or median number of trips - probably median since most months will be right-tailed. would need to see histogram of that count. then use the value to be part of the segregation amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://networkx.github.io/documentation/stable/tutorial.html use for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty multi directional graph\n",
    "#Use this to pull from dataframe into the Graph\n",
    "G=from_pandas_edgelist(df, source='source', target='target', edge_attr='weight', create_using=nx.DiGraph())\n",
    "df = name of the date frame\n",
    "source = title of column in data frame # or 'string'\n",
    "target = title of column in data frame # or 'string'\n",
    "may want the edge_attr to be the count\n",
    "edge_attr = ['weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(g)\n",
    "plt.show()\n",
    "#https://stackoverflow.com/questions/52400380/assign-edge-weights-to-a-networkx-graph-using-pandas-dataframe use to study how to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning to think that we'll need to use both nx and OSMnx. NX to do the analysis with number. OSMnx to visualize on top of a street basis? Or should I use the kepler.io, especially if station id names aren't geographically related. I'm wondering if there is a way to plot the the map with directionality - maybe I'll have to use kepler.io for that part. and leave analysis to nx?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
